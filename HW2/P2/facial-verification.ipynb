{"cells":[{"cell_type":"markdown","metadata":{"id":"UbXkUQWFLBRF"},"source":["# HW2P2: Image Recognition and Verification"]},{"cell_type":"markdown","metadata":{"id":"VMg74_LaLL55"},"source":["This is the second homework  in 11785: Introduction to Deep Learning. We are trying to tackle the problem of Image Verification. For this, we will need to first train our own CNN model to tackle the problem of classification, consisting of 8631 identities. Using this, we get the face embeddings for different pairs of images and try to identify if the pair of face matches or not."]},{"cell_type":"markdown","metadata":{"id":"oaUgdhRqy8oT"},"source":[]},{"cell_type":"markdown","metadata":{},"source":["## Running the Notebook\n","\n","- Clone or download the notebook and place it in your desired directory.\n","- Navigate to the notebook’s directory.\n","- Uncomment and run the cell labeled Kaggle if you're running from platforms other than Kaggle\n","- Extract the dataset and ensure it’s in the correct folder structure.\n","- Open the notebook in Jupyter or Colab, and run the cells sequentially.\n","\n","## Data Loading Scheme\n","\n","- Batch size     :  256\n","- Context        :  80\n","- Input Classes     :  8631\n","\n","## Architecture and Hyperparameters\n","\n","During development, multiple model architectures were tested, with several hyperparameter variations for each to \n","explore different configurations and determine the best-performing approach. Some of the initial models are included \n","in the code as commented-out sections, providing insight into alternative architectures and configurations attempted \n","during experimentation.\n","\n","Previous Archtecture Explored\n","- a custom 5 layer CNN for feature extraction for early submission\n","- the number of channels for each layer were [64,128,256,512,1024]\n","- Each layer Accompanied by a BatchNorm and ReLu activation\n","- Another architecture explored was the ResNeXt.\n","- ResNeXt is based on the concept of residual connections allowing network to learn identity mapping\n","- ResNext introduced cardinality -- a split transform merge strategy to enhance feature representation\n","\n","for the complete ablations, kindly check [this link to wandb public link](https://wandb.ai/DL_Busters/hw2p2-ablations/table?nw=nwusermabdulba)\n","\n","### Best performing architecture & model\n","\n","- I used the Sqeeze-and-Excitation ResNeXt (SEResNeXt)\n","- This archtecture adopts a Squeeze Excitation block for adaptive feature recalibration\n","- Global Average Pooling was used for summarizing global information\n","- The Specific archtecture used a 4 stage layer with each layer containing varying number of bottlenecks\n","- Similarity for verification\n","- Optimizer: Adam\n","- Metrics: Retrieval Accuracy\n","- Initial Learning Rate: 0.1\n","- Scheduler: ReduceLROnPlateau\n","- Batch Size: 256\n","- Epochs: 60\n","\n","## Submitted by: mabdulba\n"]},{"cell_type":"markdown","metadata":{"id":"695K5zs36a48"},"source":["# Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sQr0ss8w6jVI","trusted":true},"outputs":[],"source":["!nvidia-smi # Run this to see what GPU you have"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MmbTatic6PDX","trusted":true},"outputs":[],"source":["!pip install wandb --quiet # Install WandB\n","!pip install pytorch_metric_learning --quiet #Install the Pytorch Metric Library"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install torchsummary\n","# !pip install torchsummaryX==1.1.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_oCzGBTh6xjL","trusted":true},"outputs":[],"source":["import torch\n","from torchsummary import summary\n","# from torchsummaryX import summary\n","import torchvision\n","from torchvision import transforms\n","import torch.nn.functional as F\n","import os\n","import gc\n","from tqdm import tqdm\n","from PIL import Image\n","import numpy as np\n","import pandas as pd\n","from sklearn.metrics import accuracy_score\n","from sklearn import metrics as mt\n","from scipy.optimize import brentq\n","from scipy.interpolate import interp1d\n","import glob\n","import wandb\n","import math\n","import matplotlib.pyplot as plt\n","from pytorch_metric_learning import samplers\n","import csv\n","from torchvision.transforms import v2\n","from torch.utils.data import default_collate\n","\n","DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(\"Device: \", DEVICE)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MT9MZk9p69Q5","trusted":true},"outputs":[],"source":["# from google.colab import drive # Link to your drive if you are not using Colab with GCP\n","# drive.mount('/content/drive') # Models in this HW take a long time to get trained and make sure to save it here"]},{"cell_type":"markdown","metadata":{"id":"gf6Da1K37BSJ"},"source":["# Kaggle"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z1Uu5z2K7AS3","trusted":true},"outputs":[],"source":["# TODO: Use the same Kaggle code from HW1P2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sNRYbTmU7Dk3","trusted":true},"outputs":[],"source":["# # # Reminder: Make sure you have connected your kaggle API before running this block\n","# !mkdir '/content/data'\n","\n","# !kaggle competitions download -c 11785-hw-2-p-2-face-verification-fall-2024\n","# !unzip -qo '11785-hw-2-p-2-face-verification-fall-2024.zip' -d '/content/data'"]},{"cell_type":"markdown","metadata":{"id":"9OgkfYwP7HVt"},"source":["# Config"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CMXkHmFc7G9m","trusted":true},"outputs":[],"source":["config = {\n","    'batch_size': 256, # Increase this if your GPU can handle it\n","    'lr': 0.1,\n","    'epochs': 80, # 20 epochs is recommended ONLY for the early submission - you will have to train for much longer typically.\n","    'data_dir': \"/kaggle/input/11785-hw-2-p-2-face-verification-fall-2024/11-785-f24-hw2p2-verification/cls_data\", #TODO\n","    'data_ver_dir': \"/kaggle/input/11785-hw-2-p-2-face-verification-fall-2024/11-785-f24-hw2p2-verification/ver_data\", #TODO\n","    'checkpoint_dir': \"/kaggle/working/\" ,\n","    # Include other parameters as needed.\n","    'optimizer'     : 'SGD',\n","    'scheduler'     : 'CosineAnnealingLR',\n","    'patience'      : 2,\n","    'weight_decay'  : 1e-4,\n","    'momentum'      : 0.9,\n","    'resume_training': False,\n","    'model_path'     : \"/kaggle/input/seresnext/pytorch/default/1/last.pth\"\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def build_optimizer(model, optimizer: str, lr: float, weight_decay: float=0, momentum:float=0):\n","    if optimizer == 'SGD':\n","        return torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n","\n","    elif optimizer == 'Adam':\n","        return torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n","    \n","    elif optimizer == 'AdamW':\n","        weight_decay_ = weight_decay if weight_decay != 0 else 0.01\n","        return torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay_)\n","    \n","    else:\n","        raise ValueError(f'Unknown optimizer: {optimizer}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def build_scheduler(optimizer, scheduler: str, epochs: int, lr: float):\n","    if scheduler == 'CosineAnnealingLR':\n","        return torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=0.00004)\n","    elif scheduler == 'CosineAnnealingWarmRestarts':\n","        return torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, eta_min=1e-7, T_mult=2)\n","    elif scheduler == 'StepLR':\n","        return torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n","    elif scheduler == 'ReduceLROnPlateau':\n","        return torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,patience=config['patience'])\n","    else:\n","        raise ValueError(f'Unknown scheduler: {scheduler}')\n","        "]},{"cell_type":"markdown","metadata":{"id":"EEAW65sB8Wlp"},"source":["# Dataset"]},{"cell_type":"markdown","metadata":{"id":"mPSk8DyK8htk"},"source":["## Dataset Class for doing Image Verification"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KBleUieO8lwG","trusted":true},"outputs":[],"source":["class ImagePairDataset(torch.utils.data.Dataset):\n","\n","    def __init__(self, data_dir, csv_file, transform):\n","        self.data_dir = data_dir\n","        self.transform = transform\n","        self.pairs = []\n","        if csv_file.endswith('.csv'):\n","            with open(csv_file, 'r') as f:\n","                reader = csv.reader(f)\n","                for i, row in enumerate(reader):\n","                    if i == 0:\n","                        continue\n","                    else:\n","                        self.pairs.append(row)\n","        else:\n","            with open(csv_file, 'r') as f:\n","                for line in f.readlines():\n","                    self.pairs.append(line.strip().split(' '))\n","\n","    def __len__(self):\n","        return len(self.pairs)\n","\n","    def __getitem__(self, idx):\n","\n","        img_path1, img_path2, match = self.pairs[idx]\n","        img1 = Image.open(os.path.join(self.data_dir, img_path1))\n","        img2 = Image.open(os.path.join(self.data_dir, img_path2))\n","        return self.transform(img1), self.transform(img2), int(match)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xgBHYshwN9VY","trusted":true},"outputs":[],"source":["class TestImagePairDataset(torch.utils.data.Dataset):\n","\n","    def __init__(self, data_dir, csv_file, transform):\n","        self.data_dir = data_dir\n","        self.transform = transform\n","        self.pairs = []\n","        if csv_file.endswith('.csv'):\n","            with open(csv_file, 'r') as f:\n","                reader = csv.reader(f)\n","                for i, row in enumerate(reader):\n","                    if i == 0:\n","                        continue\n","                    else:\n","                        self.pairs.append(row)\n","        else:\n","            with open(csv_file, 'r') as f:\n","                for line in f.readlines():\n","                    self.pairs.append(line.strip().split(' '))\n","\n","    def __len__(self):\n","        return len(self.pairs)\n","\n","    def __getitem__(self, idx):\n","\n","        img_path1, img_path2 = self.pairs[idx]\n","        img1 = Image.open(os.path.join(self.data_dir, img_path1))\n","        img2 = Image.open(os.path.join(self.data_dir, img_path2))\n","        return self.transform(img1), self.transform(img2)"]},{"cell_type":"markdown","metadata":{"id":"2j24TXNo9P97"},"source":["## Create Dataloaders for Image Recognition"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["cutmix = v2.CutMix(num_classes=8631)\n","mixup = v2.MixUp(alpha=0.8, num_classes=8631)\n","cutmix_or_mixup = v2.RandomChoice([cutmix, mixup])\n","\n","\n","def collate_fn(batch):\n","    return cutmix_or_mixup(*default_collate(batch))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"taxotdUr9PfH","trusted":true},"outputs":[],"source":["data_dir = config['data_dir']\n","# train_dir = os.path.join(data_dir)\n","\n","train_dir = os.path.join(data_dir, 'train')\n","val_dir = os.path.join(data_dir, 'dev')\n","\n","# train transforms\n","train_transforms = torchvision.transforms.Compose([\n","    torchvision.transforms.Resize(112), # Why are we resizing the Image?\n","    torchvision.transforms.RandomRotation(degrees=30),\n","    torchvision.transforms.RandomHorizontalFlip(p=0.25),\n","    torchvision.transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1, hue=0),\n","    torchvision.transforms.ToTensor(),\n","    # torchvision.transforms.Normalize(mean=[0.5, 0.5, 0.5],\n","    #                                   std=[0.5, 0.5, 0.5])\n","])\n","\n","# val transforms\n","val_transforms = torchvision.transforms.Compose([\n","    torchvision.transforms.Resize(112),\n","    torchvision.transforms.ToTensor(),\n","    # torchvision.transforms.Normalize(mean=[0.5, 0.5, 0.5],\n","    #                                   std=[0.5, 0.5, 0.5])\n","]\n","                                               )\n","\n","\n","# get datasets\n","train_dataset = torchvision.datasets.ImageFolder(train_dir, transform=train_transforms)\n","val_dataset = torchvision.datasets.ImageFolder(val_dir, transform=val_transforms)\n","\n","train_loader = torch.utils.data.DataLoader(train_dataset,\n","                                            batch_size=config[\"batch_size\"],\n","                                            shuffle=True,\n","                                            pin_memory=True,\n","                                            num_workers=4,\n","                                            collate_fn=collate_fn,\n","                                            sampler=None)\n","val_loader = torch.utils.data.DataLoader(val_dataset,\n","                                          batch_size=config[\"batch_size\"],\n","                                          shuffle=False,\n","                                          num_workers=4)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YAsCf0Hq8pL9","trusted":true},"outputs":[],"source":["data_dir = config['data_ver_dir']\n","\n","\n","# get datasets\n","\n","# TODO: Add your validation pair txt file\n","pair_dataset = ImagePairDataset(data_dir, csv_file='/kaggle/input/11785-hw-2-p-2-face-verification-fall-2024/11-785-f24-hw2p2-verification/val_pairs.txt', transform=val_transforms)\n","pair_dataloader = torch.utils.data.DataLoader(pair_dataset,\n","                                              batch_size=config[\"batch_size\"],\n","                                              shuffle=False,\n","                                              pin_memory=True,\n","                                              num_workers=4)\n","\n","# TODO: Add your validation pair txt file\n","test_pair_dataset = TestImagePairDataset(data_dir, csv_file='/kaggle/input/11785-hw-2-p-2-face-verification-fall-2024/11-785-f24-hw2p2-verification/test_pairs.txt', transform=val_transforms)\n","test_pair_dataloader = torch.utils.data.DataLoader(test_pair_dataset,\n","                                              batch_size=config[\"batch_size\"],\n","                                              shuffle=False,\n","                                              pin_memory=True,\n","                                              num_workers=4)"]},{"cell_type":"markdown","metadata":{"id":"436KzM6u-3A2"},"source":["# EDA and Viz"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AhnoHopx-0RB","trusted":true},"outputs":[],"source":["# Double-check your dataset/dataloaders work as expected\n","\n","print(\"Number of classes    : \", len(train_dataset.classes))\n","print(\"No. of train images  : \", train_dataset.__len__())\n","print(\"Shape of image       : \", train_dataset[0][0].shape)\n","print(\"Batch size           : \", config['batch_size'])\n","print(\"Train batches        : \", train_loader.__len__())\n","print(\"Val batches          : \", val_loader.__len__())\n","\n","# Feel free to print more things if needed"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CvKiA3GR_IPO","trusted":true},"outputs":[],"source":["# Visualize a few images in the dataset\n","\n","\"\"\"\n","You can write your own code, and you don't need to understand the code\n","It is highly recommended that you visualize your data augmentation as sanity check\n","\"\"\"\n","\n","r, c    = [5, 5]\n","fig, ax = plt.subplots(r, c, figsize= (15, 15))\n","\n","k       = 0\n","dtl     = torch.utils.data.DataLoader(\n","    dataset     = torchvision.datasets.ImageFolder(train_dir, transform= train_transforms), # dont wanna see the images with transforms\n","    batch_size  = config['batch_size'],\n","    shuffle     = True)\n","\n","for data in dtl:\n","    x, y = data\n","\n","    for i in range(r):\n","        for j in range(c):\n","            img = x[k].numpy().transpose(1, 2, 0)\n","            ax[i, j].imshow(img)\n","            ax[i, j].axis('off')\n","            k+=1\n","    break\n","\n","del dtl"]},{"cell_type":"markdown","metadata":{"id":"y3TUocDw_JU_"},"source":["# Model Architecture"]},{"cell_type":"markdown","metadata":{"id":"yIR-dTHYyUov"},"source":["FAQ:\n","\n","**What's a very low early deadline architecture (mandatory early submission)**?\n","\n","- The very low early deadline architecture is a 5-layer CNN.\n","- The first convolutional layer has 64 channels, kernel size 7, and stride 4. The next three have 128, 256, 512 and 1024 channels. Each have kernel size 3 and stride 2. Documentation to make convolutional layers: https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n","- Think about strided convolutions from the lecture, as convolutions with stride = 1 and downsampling. For strided convolution, what padding do you need for preserving the spatial resolution? (Hint => padding = kernel_size // 2) - Think why?\n","- Each convolutional layer is accompanied by a Batchnorm and ReLU layer.\n","- Finally, you want to average pool over the spatial dimensions to reduce them to 1 x 1. Use AdaptiveAvgPool2d. Documentation for AdaptiveAvgPool2d: https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool2d.html\n","- Then, remove (Flatten?) these trivial 1x1 dimensions away.\n","Look through https://pytorch.org/docs/stable/nn.html\n","\n","\n","**Why does a very simple network have 4 convolutions**?\n","\n","Input images are 112x112. Note that each of these convolutions downsample. Downsampling 2x effectively doubles the receptive field, increasing the spatial region each pixel extracts features from. Downsampling 32x is standard for most image models.\n","\n","**Why does a very simple network have high channel sizes**?\n","\n","Every time you downsample 2x, you do 4x less computation (at same channel size). To maintain the same level of computation, you 2x increase # of channels, which increases computation by 4x. So, balances out to same computation. Another intuition is - as you downsample, you lose spatial information. We want to preserve some of it in the channel dimension.\n","\n","**What is return_feats?**\n","\n","It essentially returns the second-to-last-layer features of a given image. It's a \"feature encoding\" of the input image, and you can use it for the verification task. You would use the outputs of the final classification layer for the classification task. You might also find that the classification outputs are sometimes better for verification too - try both."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4LLX2Rki_LzA","trusted":true},"outputs":[],"source":["# # TODO: Fill out the model definition below\n","\n","# class Network(torch.nn.Module):\n","\n","#     def __init__(self, num_classes=8631):\n","#         super().__init__()\n","\n","#         self.backbone = torch.nn.Sequential(\n","#             # TODO\n","#             torch.nn.Conv2d(in_channels=3, out_channels=64, kernel_size=7, stride=4, padding=3),\n","#             torch.nn.BatchNorm2d(64),\n","#             torch.nn.ReLU(),\n","\n","#             torch.nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=1),\n","#             torch.nn.BatchNorm2d(128),\n","#             torch.nn.ReLU(),\n","\n","#             torch.nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1),\n","#             torch.nn.BatchNorm2d(256),\n","#             torch.nn.ReLU(),\n","            \n","#             torch.nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=2, padding=1),\n","#             torch.nn.BatchNorm2d(512),\n","#             torch.nn.ReLU(),\n","            \n","#             torch.nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, stride=2, padding=1),\n","#             torch.nn.BatchNorm2d(1024),\n","#             torch.nn.ReLU(),\n","            \n","#             torch.nn.AdaptiveAvgPool2d((1, 1))  # Reduces the spatial dimensions to 1x1\n","#             )\n","\n","#         self.cls_layer = torch.nn.Linear(1024, num_classes)\n","\n","#     def forward(self, x):\n","#             # TODO:\n","        \n","#         # Pass input through the backbone (CNN layers)\n","#         feats = self.backbone(x)\n","        \n","#         # Flatten the features before passing to the classification layer\n","#         feats = feats.view(feats.size(0), -1)  # Remove 1x1 spatial dimensions\n","        \n","#         # Pass the flattened features through the classification layer\n","#         out = self.cls_layer(feats)\n","\n","#         return {\"feats\": feats, \"out\": out}\n","\n","# # Initialize your model\n","# model = Network().to(DEVICE)\n","# summary(model, (3, 112, 112))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# # ConvNeXt Block Definition\n","# class ConvNeXtBlock(torch.nn.Module):\n","#     def __init__(self, in_channels, out_channels):\n","#         super(ConvNeXtBlock, self).__init__()\n","#         self.dw_conv = torch.nn.Conv2d(in_channels, in_channels, kernel_size=7, padding=3, groups=in_channels)  # Depthwise conv\n","#         self.bn = torch.nn.BatchNorm2d(in_channels)  # Batch Normalization\n","#         self.pointwise_conv = torch.nn.Conv2d(in_channels, out_channels, kernel_size=1)  # Pointwise conv\n","#         self.act = torch.nn.GELU()  # Activation function\n","\n","#     def forward(self, x):\n","#         x = self.dw_conv(x)         # Apply depthwise convolution\n","#         x = self.bn(x)              # Apply BatchNorm\n","#         x = self.pointwise_conv(x)  # Apply pointwise convolution\n","#         x = self.act(x)             # Apply activation\n","#         return x\n","\n","# # Complete ConvNeXt Model Definition\n","# class ConvNeXt(torch.nn.Module):\n","#     def __init__(self, num_classes=8631):\n","#         super(ConvNeXt, self).__init__()\n","        \n","#         # Stem Layer\n","#         self.stem = torch.nn.Sequential(\n","#             torch.nn.Conv2d(3, 64, kernel_size=7, stride=4, padding=3),  # Downsample input\n","#             torch.nn.BatchNorm2d(64),\n","#             torch.nn.ReLU(inplace=True)\n","#         )\n","        \n","#         # ConvNeXt Stages\n","#         self.stage1 = ConvNeXtBlock(64, 256)  # First stage\n","#         self.stage2 = ConvNeXtBlock(256, 512)  # Second stage\n","#         self.stage3 = ConvNeXtBlock(512, 1024)  # Third stage\n","#         self.stage4 = ConvNeXtBlock(1024, 2058)  # Fourth stage\n","\n","#         # Global Pooling and Classification Layer\n","#         self.avgpool = torch.nn.AdaptiveAvgPool2d((1, 1))  # Global Average Pooling\n","#         self.flatten = torch.nn.Flatten()\n","#         self.cls_layer = torch.nn.Linear(2058, num_classes)  # Classification layer\n","\n","#     def forward(self, x):\n","#         x = self.stem(x)       # Initial downsampling\n","#         x = self.stage1(x)     # Pass through ConvNeXt blocks\n","#         x = self.stage2(x)\n","#         x = self.stage3(x)\n","#         x = self.stage4(x)\n","#         x = self.avgpool(x)    # Global pooling to reduce spatial size to (1, 1)\n","#         x = self.flatten(x)    # Flatten the output\n","#         out = self.cls_layer(x)  # Classification layer\n","#         return {\"feats\": x, \"out\": out}\n","\n","# # Initialize the ConvNeXt model\n","# model = ConvNeXt(num_classes=8631).to(DEVICE)\n","\n","\n","# # Using your input data\n","# input_data = torch.zeros((1, 3, 112, 112)).to(DEVICE)  \n","\n","# # Ensure you pass a tensor and not a tuple\n","# summary(model, input_data)  # Pass the tensor directly for summary\n","\n","\n","# model = torch.nn.DataParallel(model).to(DEVICE)"]},{"cell_type":"markdown","metadata":{},"source":["**#SEResNet**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# class SqueezeExcitationBlock(torch.nn.Module):\n","#     def __init__(self, in_channels, reduction_ratio=16):\n","#         super(SqueezeExcitationBlock, self).__init__()\n","        \n","#         self.global_avg_pool = torch.nn.AdaptiveAvgPool2d(1)\n","#         self.fc1 = torch.nn.Linear(in_channels, in_channels // reduction_ratio)\n","        \n","#         self.relu = torch.nn.ReLU(inplace=True)\n","#         self.fc2 = torch.nn.Linear(in_channels // reduction_ratio, in_channels)\n","        \n","#         self.sigmoid = torch.nn.Sigmoid()\n","\n","#     def forward(self, x):\n","#         out = self.global_avg_pool(x).squeeze(-1).squeeze(-1)\n","#         out = self.fc1(out)\n","        \n","#         out = self.relu(out)\n","#         out = self.fc2(out)\n","        \n","#         out = self.sigmoid(out)\n","#         out = out.unsqueeze(-1).unsqueeze(-1)\n","#         return x * out\n","\n","# class BasicBlock(torch.nn.Module):\n","#     def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n","#         super(BasicBlock, self).__init__()\n","        \n","#         self.conv1 = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n","#         self.bn1 = torch.nn.BatchNorm2d(out_channels)\n","        \n","#         self.relu = torch.nn.ReLU(inplace=True)\n","#         self.conv2 = torch.nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n","        \n","#         self.bn2 = torch.nn.BatchNorm2d(out_channels)\n","#         self.se_block = SqueezeExcitationBlock(out_channels)\n","        \n","#         self.downsample = downsample\n","\n","#     def forward(self, x):\n","#         residual = x\n","\n","#         out = self.conv1(x)\n","#         out = self.bn1(out)\n","#         out = self.relu(out)\n","\n","#         out = self.conv2(out)\n","#         out = self.bn2(out)\n","\n","#         out = self.se_block(out)\n","\n","#         if self.downsample is not None:\n","#             residual = self.downsample(x)\n","\n","#         out += residual\n","#         out = self.relu(out)\n","\n","#         return out\n","\n","# class SEResNet(torch.nn.Module):\n","#     def __init__(self, block, layers, num_classes=8631):\n","#         super(SEResNet, self).__init__()\n","#         self.in_channels = 64\n","#         self.conv1 = torch.nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n","#         self.bn1 = torch.nn.BatchNorm2d(64)\n","#         self.relu = torch.nn.ReLU(inplace=True)\n","#         self.maxpool = torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","\n","#         self.layer1 = self._make_layer(block, 64, layers[0])\n","#         self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n","#         self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n","#         self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n","\n","#         self.global_avg_pool = torch.nn.AdaptiveAvgPool2d(1)\n","#         self.fc = torch.nn.Linear(512, num_classes)\n","\n","#     def _make_layer(self, block, out_channels, blocks, stride=1):\n","#         downsample = None\n","#         if stride != 1 or self.in_channels != out_channels:\n","#             downsample = torch.nn.Sequential(\n","#                 torch.nn.Conv2d(self.in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n","#                 torch.nn.BatchNorm2d(out_channels)\n","#             )\n","\n","#         layers = [block(self.in_channels, out_channels, stride, downsample)]\n","#         self.in_channels = out_channels\n","\n","#         for _ in range(1, blocks):\n","#             layers.append(block(out_channels, out_channels))\n","\n","#         return torch.nn.Sequential(*layers)\n","\n","#     def forward(self, x, return_feats=True):\n","#         x = self.conv1(x)\n","#         x = self.bn1(x)\n","#         x = self.relu(x)\n","#         x = self.maxpool(x)\n","\n","#         x = self.layer1(x)\n","#         x = self.layer2(x)\n","#         x = self.layer3(x)\n","#         x = self.layer4(x)\n","\n","#         x = self.global_avg_pool(x)\n","        \n","#         x_feats = x.view(x.size(0), -1)\n","#         x = self.fc(x_feats)\n","        \n","#         return {\"feats\": x_feats, \"out\": x}\n","\n","\n","\n","\n","# # Initialize your model\n","# model = SEResNet(BasicBlock, [4, 5, 6, 2], num_classes=8631)\n","\n","# model = torch.nn.DataParallel(model).to(DEVICE)\n","\n","\n","# summary(model.module, (3, 112, 112))"]},{"cell_type":"markdown","metadata":{},"source":["**# SE-ResNeXt**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class Selayer(torch.nn.Module):\n","\n","    def __init__(self, inplanes):\n","        super(Selayer, self).__init__()\n","        self.global_avgpool = torch.nn.AdaptiveAvgPool2d(1)\n","        self.conv1 = torch.nn.Conv2d(inplanes, inplanes // 16, kernel_size=1, stride=1)\n","        self.conv2 = torch.nn.Conv2d(inplanes // 16, inplanes, kernel_size=1, stride=1)\n","        self.relu = torch.nn.ReLU(inplace=True)\n","        self.sigmoid = torch.nn.Sigmoid()\n","\n","    def forward(self, x):\n","\n","        out = self.global_avgpool(x)\n","\n","        out = self.conv1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.sigmoid(out)\n","\n","        return x * out\n","\n","\n","class Bottleneck(torch.nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, inplanes, planes, cardinality, stride=1, downsample=None):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = torch.nn.Conv2d(inplanes, planes * 2, kernel_size=1, bias=False)\n","        self.bn1 = torch.nn.BatchNorm2d(planes * 2)\n","\n","        self.conv2 = torch.nn.Conv2d(planes * 2, planes * 2, kernel_size=3, stride=stride,\n","                               padding=1, groups=cardinality, bias=False)\n","        self.bn2 = torch.nn.BatchNorm2d(planes * 2)\n","\n","        self.conv3 = torch.nn.Conv2d(planes * 2, planes * 4, kernel_size=1, bias=False)\n","        self.bn3 = torch.nn.BatchNorm2d(planes * 4)\n","\n","        self.selayer = Selayer(planes * 4)\n","\n","        self.relu = torch.nn.ReLU(inplace=True)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","    def forward(self, x):\n","        residual = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","        out = self.relu(out)\n","\n","        out = self.conv3(out)\n","        out = self.bn3(out)\n","\n","        out = self.selayer(out)\n","\n","        if self.downsample is not None:\n","            residual = self.downsample(x)\n","\n","        out += residual\n","        out = self.relu(out)\n","\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class SeResNeXt(torch.nn.Module):\n","\n","    def __init__(self, block, layers, cardinality=32, num_classes=8631):\n","        super(SeResNeXt, self).__init__()\n","        self.cardinality = cardinality\n","        self.inplanes = 64\n","\n","        self.conv1 = torch.nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n","                               bias=False)\n","        self.bn1 = torch.nn.BatchNorm2d(64)\n","        self.relu = torch.nn.ReLU(inplace=True)\n","        self.maxpool = torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","\n","        self.layer1 = self._make_layer(block, 64, layers[0])\n","        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n","\n","        self.avgpool = torch.nn.AdaptiveAvgPool2d(1)\n","        self.fc = torch.nn.Linear(512 * block.expansion, num_classes)\n","\n","        for m in self.modules():\n","            if isinstance(m, torch.nn.Conv2d):\n","                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","                m.weight.data.normal_(0, math.sqrt(2. / n))\n","                if m.bias is not None:\n","                    m.bias.data.zero_()\n","            elif isinstance(m, torch.nn.BatchNorm2d):\n","                m.weight.data.fill_(1)\n","                m.bias.data.zero_()\n","\n","    def _make_layer(self, block, planes, blocks, stride=1):\n","        downsample = None\n","        if stride != 1 or self.inplanes != planes * block.expansion:\n","            downsample = torch.nn.Sequential(\n","                torch.nn.Conv2d(self.inplanes, planes * block.expansion,\n","                          kernel_size=1, stride=stride, bias=False),\n","                torch.nn.BatchNorm2d(planes * block.expansion),\n","            )\n","\n","        layers = []\n","        layers.append(block(self.inplanes, planes, self.cardinality, stride, downsample))\n","        self.inplanes = planes * block.expansion\n","        for i in range(1, blocks):\n","            layers.append(block(self.inplanes, planes, self.cardinality))\n","\n","        return torch.nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        x = self.maxpool(x)\n","\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.layer4(x)\n","\n","        # x = self.avgpool(x)\n","        # x = x.view(x.size(0), -1)\n","\n","        x = self.avgpool(x)\n","        x_feats = x.view(x.size(0), -1)\n","\n","        x = self.fc(x_feats)\n","        return {\"feats\": x_feats, \"out\":x}"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def se_resnext50(**kwargs):\n","\n","    model = SeResNeXt(Bottleneck, [3, 4, 6, 3], **kwargs)\n","    return model\n","\n","\n","def se_resnext101(**kwargs):\n","    \n","    model = SeResNeXt(Bottleneck, [3, 4, 23, 3], **kwargs)\n","    return model\n","\n","\n","def se_resnext152(**kwargs):\n","    \n","    model = SeResNeXt(Bottleneck, [3, 8, 36, 3], **kwargs)\n","    return model\n","\n","def se_resnext_custom(**kwargs):\n","\n","    model = SeResNeXt(Bottleneck, [4, 5, 6, 2], **kwargs)\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Initialize your model\n","model = se_resnext_custom(num_classes=8631)\n","\n","model = torch.nn.DataParallel(model).to(DEVICE)\n","\n","\n","summary(model.module, (3, 112, 112))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pDP--pND_3Vy","trusted":true},"outputs":[],"source":["# --------------------------------------------------- #\n","\n","# Defining Loss function\n","criterion = torch.nn.CrossEntropyLoss(label_smoothing=0.2) # TODO: What loss do you need for a multi class classification problem and would label smoothing be beneficial here?\n","\n","# --------------------------------------------------- #\n","\n","# Defining Optimizer\n","# optimizer = torch.optim.SGD(model.parameters(), lr=config[\"lr\"], momentum=0.9) # TODO: Feel free to pick a optimizer\n","optimizer = build_optimizer(model, optimizer=config[\"optimizer\"], lr=config[\"lr\"], weight_decay=config['weight_decay'], momentum=config['momentum'])\n","# --------------------------------------------------- #\n","\n","# Defining Scheduler\n","# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=config['patience'])  # TODO: Use a good scheduler such as ReduceLRonPlateau, StepLR, MultistepLR, CosineAnnealing, etc.\n","# scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, 10, T_mult=2, eta_min=1e-6) \n","# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, config[\"epochs\"], eta_min=1e-7)\n","\n","scheduler = build_scheduler(optimizer, scheduler=config[\"scheduler\"], epochs=config[\"epochs\"], lr=config[\"lr\"])\n","# --------------------------------------------------- #\n","\n","# Initialising mixed-precision training. # Good news. We've already implemented FP16 (Mixed precision training) for you\n","# It is useful only in the case of compatible GPUs such as T4/V100\n","scaler = torch.cuda.amp.GradScaler()"]},{"cell_type":"markdown","metadata":{"id":"-d5ZDQfpw7gR"},"source":["# Metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Ecg0J2sw9jJ","trusted":true},"outputs":[],"source":["class AverageMeter:\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TqVw0ab0xBKT","trusted":true},"outputs":[],"source":["def accuracy(output, target, topk=(1,)):\n","    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n","    maxk = min(max(topk), output.size()[1])\n","    batch_size = target.size(0)\n","    _, pred = output.topk(maxk, 1, True, True)\n","    pred = pred.t()\n","    correct = pred.eq(target.reshape(1, -1).expand_as(pred))\n","    return [correct[:min(k, maxk)].reshape(-1).float().sum(0) * 100. / batch_size for k in topk]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uNCQjz2RxD5S","trusted":true},"outputs":[],"source":["def get_ver_metrics(labels, scores, FPRs):\n","    # eer and auc\n","    fpr, tpr, _ = mt.roc_curve(labels, scores, pos_label=1)\n","    roc_curve = interp1d(fpr, tpr)\n","    EER = 100. * brentq(lambda x : 1. - x - roc_curve(x), 0., 1.)\n","    AUC = 100. * mt.auc(fpr, tpr)\n","\n","    # get acc\n","    tnr = 1. - fpr\n","    pos_num = labels.count(1)\n","    neg_num = labels.count(0)\n","    ACC = 100. * max(tpr * pos_num + tnr * neg_num) / len(labels)\n","\n","    # TPR @ FPR\n","    if isinstance(FPRs, list):\n","        TPRs = [\n","            ('TPR@FPR={}'.format(FPR), 100. * roc_curve(float(FPR)))\n","            for FPR in FPRs\n","        ]\n","    else:\n","        TPRs = []\n","\n","    return {\n","        'ACC': ACC,\n","        'EER': EER,\n","        'AUC': AUC,\n","        'TPRs': TPRs,\n","    }"]},{"cell_type":"markdown","metadata":{"id":"juUbZnP0AEUi"},"source":["# Train and Validation Function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IMnxvQT-AHsu","trusted":true},"outputs":[],"source":["def train_epoch(model, dataloader, optimizer, lr_scheduler, scaler, device, config):\n","\n","    model.train()\n","\n","    # metric meters\n","    loss_m = AverageMeter()\n","    acc_m = AverageMeter()\n","\n","    # Progress Bar\n","    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, leave=False, position=0, desc='Train', ncols=5)\n","\n","    for i, (images, labels) in enumerate(dataloader):\n","\n","        optimizer.zero_grad() # Zero gradients\n","\n","        # send to cuda\n","        images = images.to(device, non_blocking=True)\n","        if isinstance(labels, (tuple, list)):\n","            targets1, targets2, lam = labels\n","            labels = (targets1.to(device), targets2.to(device), lam)\n","        else:\n","            labels = labels.to(device, non_blocking=True)\n","\n","        # forward\n","        with torch.cuda.amp.autocast():  # This implements mixed precision. Thats it!\n","            outputs = model(images)\n","            \n","            \n","\n","            # Use the type of output depending on the loss function you want to \n","            if labels.shape == outputs['out'].shape:\n","                labels = torch.argmax(labels, dim=1)\n","                \n","            loss = criterion(outputs['out'], labels )\n","\n","        scaler.scale(loss).backward() # This is a replacement for loss.backward()\n","        scaler.step(optimizer) # This is a replacement for optimizer.step()\n","        scaler.update()\n","        # metrics\n","        loss_m.update(loss.item())\n","        if 'feats' in outputs:\n","            acc = accuracy(outputs['out'], labels)[0].item()\n","        else:\n","            acc = 0.0\n","        acc_m.update(acc)\n","\n","        # tqdm lets you add some details so you can monitor training as you train.\n","        batch_bar.set_postfix(\n","            # acc         = \"{:.04f}%\".format(100*accuracy),\n","            acc=\"{:.04f}% ({:.04f})\".format(acc, acc_m.avg),\n","            loss        = \"{:.04f} ({:.04f})\".format(loss.item(), loss_m.avg),\n","            lr          = \"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])))\n","\n","        batch_bar.update() # Update tqdm bar\n","\n","    # You may want to call some schedulers inside the train function. What are these?\n","    if lr_scheduler is not None:\n","        lr_scheduler.step(loss)\n","\n","    batch_bar.close()\n","\n","    return acc_m.avg, loss_m.avg"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5qkdH295wNUX","trusted":true},"outputs":[],"source":["@torch.no_grad()\n","def valid_epoch_cls(model, dataloader, device, config):\n","\n","    model.eval()\n","    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc='Val Cls.', ncols=5)\n","\n","    # metric meters\n","    loss_m = AverageMeter()\n","    acc_m = AverageMeter()\n","\n","    for i, (images, labels) in enumerate(dataloader):\n","\n","        # Move images to device\n","        images, labels = images.to(device), labels.to(device)\n","\n","        # Get model outputs\n","        with torch.inference_mode():\n","            outputs = model(images)\n","            loss = criterion(outputs['out'], labels)\n","\n","        # metrics\n","        acc = accuracy(outputs['out'], labels)[0].item()\n","        loss_m.update(loss.item())\n","        acc_m.update(acc)\n","\n","        batch_bar.set_postfix(\n","            acc         = \"{:.04f}% ({:.04f})\".format(acc, acc_m.avg),\n","            loss        = \"{:.04f} ({:.04f})\".format(loss.item(), loss_m.avg))\n","\n","        batch_bar.update()\n","\n","    batch_bar.close()\n","    return acc_m.avg, loss_m.avg"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-Yan5vLDyj-3","trusted":true},"outputs":[],"source":["gc.collect() # These commands help you when you face CUDA OOM error\n","torch.cuda.empty_cache()"]},{"cell_type":"markdown","metadata":{"id":"0q1gRMAsyknz"},"source":["# Verification Task"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SSGeDCi-wa1W","trusted":true},"outputs":[],"source":["def valid_epoch_ver(model, pair_data_loader, device, config):\n","\n","    model.eval()\n","    scores = []\n","    match_labels = []\n","    batch_bar = tqdm(total=len(pair_data_loader), dynamic_ncols=True, position=0, leave=False, desc='Val Veri.')\n","    for i, (images1, images2, labels) in enumerate(pair_data_loader):\n","\n","        # match_labels = match_labels.to(device)\n","        images = torch.cat([images1, images2], dim=0).to(device)\n","        # Get model outputs\n","        with torch.inference_mode():\n","            outputs = model(images)\n","\n","        feats = F.normalize(outputs['feats'], dim=1)\n","        feats1, feats2 = feats.chunk(2)\n","        similarity = F.cosine_similarity(feats1, feats2)\n","        scores.append(similarity.cpu().numpy())\n","        match_labels.append(labels.cpu().numpy())\n","        batch_bar.update()\n","\n","    scores = np.concatenate(scores)\n","    match_labels = np.concatenate(match_labels)\n","\n","    FPRs=['1e-4', '5e-4', '1e-3', '5e-3', '5e-2']\n","    metric_dict = get_ver_metrics(match_labels.tolist(), scores.tolist(), FPRs)\n","    print(metric_dict)\n","\n","    return metric_dict['ACC'], metric_dict['EER']"]},{"cell_type":"markdown","metadata":{"id":"piblCbe5yotj"},"source":["# WandB"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HTIkCXBQyoM0","trusted":true},"outputs":[],"source":["# wandb.login(key=\"4da7c51b7cc783a566fb240ea3dcb4bf061cec8e\") \n","wandb.login(key=\"7eebdcce14bbf616d584f912c3f6d9d9bc706e42\") # API Key is in your wandb account, under settings (wandb.ai/settings)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GLNNqwV4ysNP","trusted":true},"outputs":[],"source":["# Create your wandb run\n","run = wandb.init(\n","    name = \"SEResNeXt-MH-with-CutMix-No-Normalization-80\", ## Wandb creates random run names if you skip this field\n","    reinit = True, ### Allows reinitalizing runs when you re-run this cell\n","    # run_id = ### Insert specific run id here if you want to resume a previous run\n","    # resume = \"must\", ### You need this to resume previous runs, but comment out reinit = True when using this\n","    project = \"hw2p2-ablations\", ### Project should be created in your wandb account\n","    config = config ### Wandb Config for your run\n",")"]},{"cell_type":"markdown","metadata":{"id":"t0RrtpFKzH3k"},"source":["# Checkpointing and Loading Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dDFmC8hpzLOq","trusted":true},"outputs":[],"source":["# Uncomment the line for saving the scheduler save dict if you are using a scheduler\n","def save_model(model, optimizer, scheduler, metrics, epoch, path):\n","    torch.save(\n","        {'model_state_dict'         : model.state_dict(),\n","         'optimizer_state_dict'     : optimizer.state_dict(),\n","         # 'scheduler_state_dict'     : scheduler.state_dict(),\n","         'metric'                   : metrics,\n","         'epoch'                    : epoch},\n","         path)\n","\n","\n","def load_model(model, optimizer=None, scheduler=None, path='./checkpoint.pth'):\n","    checkpoint = torch.load(path)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    if optimizer is not None:\n","        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","    else:\n","        optimizer = None\n","    if scheduler is not None:\n","        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n","    else:\n","        scheduler = None\n","    epoch = checkpoint['epoch']\n","    metrics = checkpoint['metric']\n","    return model, optimizer, scheduler, epoch, metrics"]},{"cell_type":"markdown","metadata":{"id":"wpFT7iriy5bi"},"source":["# Experiments"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"59FcCeJfy3Zm","trusted":true},"outputs":[],"source":["e = 0\n","best_valid_cls_acc = 0.0\n","eval_cls = True\n","best_valid_ret_acc = 0.0\n","\n","resume_training = config['resume_training']\n","\n","if resume_training:\n","    model, optimizer, scheduler, epoch, metrics = load_model(model, optimizer=optimizer, path=config['model_path'])\n","    e = epoch\n","\n","for epoch in range(e, config['epochs']):\n","        # epoch\n","        print(\"\\nEpoch {}/{}\".format(epoch+1, config['epochs']))\n","\n","        # train\n","        train_cls_acc, train_loss = train_epoch(model, train_loader, optimizer, scheduler, scaler, DEVICE, config)\n","        curr_lr = float(optimizer.param_groups[0]['lr'])\n","        print(\"\\nEpoch {}/{}: \\nTrain Cls. Acc {:.04f}%\\t Train Cls. Loss {:.04f}\\t Learning Rate {:.04f}\".format(epoch + 1, config['epochs'], train_cls_acc, train_loss, curr_lr))\n","        metrics = {\n","            'train_cls_acc': train_cls_acc,\n","            'train_loss': train_loss,\n","        }\n","        # classification validation\n","        if eval_cls:\n","            valid_cls_acc, valid_loss = valid_epoch_cls(model, val_loader, DEVICE, config)\n","            print(\"Val Cls. Acc {:.04f}%\\t Val Cls. Loss {:.04f}\".format(valid_cls_acc, valid_loss))\n","            metrics.update({\n","                'valid_cls_acc': valid_cls_acc,\n","                'valid_loss': valid_loss,\n","            })\n","\n","        # retrieval validation\n","        valid_ret_acc, eer = valid_epoch_ver(model, pair_dataloader, DEVICE, config)\n","        print(\"Val Ret. Acc {:.04f}%\".format(valid_ret_acc))\n","        metrics.update({\n","            'valid_ret_acc': valid_ret_acc,\n","            \"EER\"          : eer\n","        })\n","\n","        # save model\n","        save_model(model, optimizer, scheduler, metrics, epoch, os.path.join(config['checkpoint_dir'], 'last.pth'))\n","        wandb.save(os.path.join(config['checkpoint_dir'], 'last.pth'))\n","        print(\"Saved epoch model\")\n","\n","        # save best model\n","        if eval_cls:\n","            if valid_cls_acc >= best_valid_cls_acc:\n","                best_valid_cls_acc = valid_cls_acc\n","                save_model(model, optimizer, scheduler, metrics, epoch, os.path.join(config['checkpoint_dir'], 'best_cls.pth'))\n","                wandb.save(os.path.join(config['checkpoint_dir'], 'best_cls.pth'))\n","                print(\"Saved best classification model\")\n","\n","        if valid_ret_acc >= best_valid_ret_acc:\n","            best_valid_ret_acc = valid_ret_acc\n","            save_model(model, optimizer, scheduler, metrics, epoch, os.path.join(config['checkpoint_dir'], 'best_ret.pth'))\n","            wandb.save(os.path.join(config['checkpoint_dir'], 'best_ret.pth'))\n","            print(\"Saved best retrieval model\")\n","\n","        # log to tracker\n","        if run is not None:\n","            run.log(metrics)"]},{"cell_type":"markdown","metadata":{"id":"7aZ7yRdBKraO"},"source":["# Testing and Kaggle Submission (Verification)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XUAa3m2h0eCD","trusted":true},"outputs":[],"source":["def test_epoch_ver(model, pair_data_loader, config):\n","\n","    model.eval()\n","    scores = []\n","    batch_bar = tqdm(total=len(pair_data_loader), dynamic_ncols=True, position=0, leave=False, desc='Val Veri.')\n","    for i, (images1, images2) in enumerate(pair_data_loader):\n","\n","        images = torch.cat([images1, images2], dim=0).to(DEVICE)\n","        # Get model outputs\n","        with torch.inference_mode():\n","            outputs = model(images)\n","\n","        feats = F.normalize(outputs['feats'], dim=1)\n","        feats1, feats2 = feats.chunk(2)\n","        similarity = F.cosine_similarity(feats1, feats2)\n","        scores.extend(similarity.cpu().numpy().tolist())\n","        batch_bar.update()\n","\n","    return scores"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# model, optimizer, scheduler, epoch, metrics = load_model(model, optimizer=optimizer, path=config['model_path'])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KZob_EOvIb65","outputId":"aec87391-d825-4eb4-f439-7a97681dbd39","trusted":true},"outputs":[],"source":["scores = test_epoch_ver(model, test_pair_dataloader, config)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ETSImix3AYy_","trusted":true},"outputs":[],"source":["with open(\"verification_submission_slack.csv\", \"w+\") as f:\n","    f.write(\"ID,Label\\n\")\n","    for i in range(len(scores)):\n","        f.write(\"{},{}\\n\".format(i, scores[i]))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":9626915,"sourceId":83922,"sourceType":"competition"}],"dockerImageVersionId":30787,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
